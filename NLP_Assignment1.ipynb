{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIgM6C9HYUhm"
      },
      "source": [
        "# Context-sensitive Spelling Correction\n",
        "\n",
        "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
        "\n",
        "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
        "\n",
        "Useful links:\n",
        "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
        "- [Norvig's dataset](https://norvig.com/big.txt)\n",
        "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
        "\n",
        "Grading:\n",
        "- 60 points - Implement spelling correction\n",
        "- 20 points - Justify your decisions\n",
        "- 20 points - Evaluate on a test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-vb8yFOGRDF"
      },
      "source": [
        "## Implement context-sensitive spelling correction\n",
        "\n",
        "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
        "\n",
        "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
        "\n",
        "When solving this task, we expect you'll face (and successfully deal with) some problems or make up the ideas of the model improvement. Some of them are: \n",
        "\n",
        "- solving a problem of n-grams frequencies storing for a large corpus;\n",
        "- taking into account keyboard layout and associated misspellings;\n",
        "- efficiency improvement to make the solution faster;\n",
        "- ...\n",
        "\n",
        "Please don't forget to describe such cases, and what you decided to do with them, in the Justification section.\n",
        "\n",
        "##### IMPORTANT:  \n",
        "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
        "- Your implementation\n",
        "- Analysis of why the implemented approach is suggested\n",
        "- Improvements of the original approach that you have chosen to implement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "from collections import defaultdict\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Convert the input text to lowercase and split it into tokens.\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The input string.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of lowercase tokens.\n",
        "    \"\"\"\n",
        "    return text.lower().split()\n",
        "\n",
        "def get_ngrams(corpus, n, smoothing_factor=1):\n",
        "    \"\"\"\n",
        "    Compute the frequency of n-grams from a corpus of texts using simple Laplace smoothing.\n",
        "\n",
        "    Parameters:\n",
        "        corpus (List[str]): A list of text lines.\n",
        "        n (int): The n-gram order (e.g., 1 for unigrams, 2 for bigrams).\n",
        "        smoothing_factor (int, optional): Smoothing factor added to each observed n-gram count. Default is 1.\n",
        "\n",
        "    Returns:\n",
        "        tuple:\n",
        "            - ngrams (defaultdict): A dictionary with n-gram tuples as keys and their (smoothed) frequency as values.\n",
        "            - total_ngrams (int): Total number of n-grams in the corpus.\n",
        "    \"\"\"\n",
        "    ngrams = defaultdict(int)\n",
        "    total_ngrams = 0\n",
        "\n",
        "    for line in corpus:\n",
        "        # Add start and end tokens to the sentence\n",
        "        tokens = [\"<s>\"] + preprocess_text(line) + [\"</s>\"]\n",
        "        for i in range(len(tokens) - n + 1):\n",
        "            ngram = tuple(tokens[i:i+n])\n",
        "            ngrams[ngram] += 1\n",
        "            total_ngrams += 1\n",
        "\n",
        "    # Apply Laplace smoothing to observed n-grams\n",
        "    for ngram in ngrams:\n",
        "        ngrams[ngram] += smoothing_factor\n",
        "\n",
        "    return ngrams, total_ngrams\n",
        "\n",
        "def edits1(word):\n",
        "    \"\"\"\n",
        "    Generate a set of words that are one edit away from the input word.\n",
        "    The possible edits include insertion, deletion, substitution, and transposition of characters.\n",
        "\n",
        "    Parameters:\n",
        "        word (str): The original word.\n",
        "\n",
        "    Returns:\n",
        "        Set[str]: A set of candidate words that are one edit distance away.\n",
        "    \"\"\"\n",
        "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
        "    deletes = [L + R[1:] for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
        "    replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
        "    inserts = [L + c + R for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def get_candidates(word, vocabulary, keyboard_layout=None):\n",
        "    \"\"\"\n",
        "    Generate candidate corrections for a given word.\n",
        "\n",
        "    - If the word is already present in the vocabulary, it is considered correct.\n",
        "    - Otherwise, candidates with edit distance 1 are generated, and if none are found,\n",
        "      candidates with edit distance 2 are considered.\n",
        "    - If a keyboard layout is provided, candidates that account for common keyboard errors are also included.\n",
        "\n",
        "    Parameters:\n",
        "        word (str): The word to be corrected.\n",
        "        vocabulary (set): A set of known (valid) words.\n",
        "        keyboard_layout (List[str], optional): List of keyboard rows (e.g., [\"qwertyuiop\"]) to simulate adjacent key errors. Default is None.\n",
        "\n",
        "    Returns:\n",
        "        Set[str]: A set of candidate corrections.\n",
        "    \"\"\"\n",
        "    candidates = set()\n",
        "\n",
        "    # If the word exists in the vocabulary, it is already correct.\n",
        "    if word in vocabulary:\n",
        "        candidates.add(word)\n",
        "    else:\n",
        "        # Generate candidates with edit distance 1.\n",
        "        candidates = edits1(word) & vocabulary\n",
        "        # If no candidates found at distance 1, try edit distance 2.\n",
        "        if not candidates:\n",
        "            candidates = {e2 for e1 in edits1(word) for e2 in edits1(e1) if e2 in vocabulary}\n",
        "\n",
        "    # Incorporate keyboard layout errors if provided.\n",
        "    if keyboard_layout:\n",
        "        kb_candidates = handle_keyboard_errors(word, keyboard_layout)\n",
        "        candidates |= (kb_candidates & vocabulary)\n",
        "\n",
        "    # If no candidate is found, return the original word.\n",
        "    return candidates if candidates else {word}\n",
        "\n",
        "def handle_keyboard_errors(word, keyboard_layout):\n",
        "    \"\"\"\n",
        "    Handle typical keyboard layout errors by considering adjacent keys.\n",
        "\n",
        "    Parameters:\n",
        "        word (str): The input word that might contain a keyboard error.\n",
        "        keyboard_layout (List[str]): A list of strings where each string represents a row on the keyboard (e.g., \"qwertyuiop\").\n",
        "\n",
        "    Returns:\n",
        "        Set[str]: A set of candidate words generated by substituting a character with its neighbor.\n",
        "    \"\"\"\n",
        "    candidates = set()\n",
        "    # Build a mapping from character to its row and index in that row.\n",
        "    layout_dict = {}\n",
        "    for row in keyboard_layout:\n",
        "        for i, char in enumerate(row):\n",
        "            layout_dict[char] = (row, i)\n",
        "\n",
        "    for i in range(len(word)):\n",
        "        if word[i] in layout_dict:\n",
        "            row, col = layout_dict[word[i]]\n",
        "            # Try replacing the character with its immediate neighbor on the left or right.\n",
        "            for delta in [-1, 1]:\n",
        "                new_col = col + delta\n",
        "                if 0 <= new_col < len(row):\n",
        "                    candidate = word[:i] + row[new_col] + word[i+1:]\n",
        "                    candidates.add(candidate)\n",
        "    return candidates\n",
        "\n",
        "def probability_of_correction(candidate, context, unigrams, bigrams, trigrams, unigram_total, smoothing_factor=1, weights=(0.1, 0.3, 0.6)):\n",
        "    \"\"\"\n",
        "    Calculate the weighted log probability of a candidate word given its context using interpolated n-gram models.\n",
        "\n",
        "    The overall score is a weighted sum of the log probabilities from:\n",
        "      - Unigram model\n",
        "      - Bigram model (if previous context exists)\n",
        "      - Trigram model (if two preceding words exist)\n",
        "\n",
        "    Parameters:\n",
        "        candidate (str): The candidate word.\n",
        "        context (List[str]): The list of preceding words (context).\n",
        "        unigrams (dict): Unigram frequency dictionary.\n",
        "        bigrams (dict): Bigram frequency dictionary.\n",
        "        trigrams (dict): Trigram frequency dictionary.\n",
        "        unigram_total (int): Total count of unigrams.\n",
        "        smoothing_factor (int, optional): Smoothing factor for Laplace smoothing. Default is 1.\n",
        "        weights (tuple, optional): Weights for (unigram, bigram, trigram) models. Default is (0.1, 0.3, 0.6).\n",
        "\n",
        "    Returns:\n",
        "        float: The interpolated log probability score for the candidate.\n",
        "    \"\"\"\n",
        "    # Calculate unigram probability\n",
        "    unigram_prob = (unigrams.get((candidate,), 0) + smoothing_factor) / (unigram_total + smoothing_factor * len(unigrams))\n",
        "    score = weights[0] * math.log(unigram_prob)\n",
        "\n",
        "    # Calculate bigram probability if previous context exists\n",
        "    if len(context) >= 1:\n",
        "        bigram_prob = (bigrams.get((context[-1], candidate), 0) + smoothing_factor) / (unigrams.get((context[-1],), 0) + smoothing_factor * len(unigrams))\n",
        "        score += weights[1] * math.log(bigram_prob)\n",
        "\n",
        "    # Calculate trigram probability if two preceding words exist\n",
        "    if len(context) >= 2:\n",
        "        trigram_prob = (trigrams.get((context[-2], context[-1], candidate), 0) + smoothing_factor) / (bigrams.get((context[-2], context[-1]), 0) + smoothing_factor * len(bigrams))\n",
        "        score += weights[2] * math.log(trigram_prob)\n",
        "\n",
        "    return score\n",
        "\n",
        "def correct_spelling(text, unigrams, bigrams, trigrams, vocabulary, unigram_total, keyboard_layout=None, smoothing_factor=1, weights=(0.1, 0.3, 0.6)):\n",
        "    \"\"\"\n",
        "    Corrects spelling errors in a given text using context-sensitive n-gram language models and candidate generation.\n",
        "\n",
        "    For each word in the input text, the function generates candidate corrections and selects the one with the highest\n",
        "    interpolated log probability score based on unigram, bigram, and trigram models.\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The input text to be corrected.\n",
        "        unigrams (dict): Unigram frequency dictionary.\n",
        "        bigrams (dict): Bigram frequency dictionary.\n",
        "        trigrams (dict): Trigram frequency dictionary.\n",
        "        vocabulary (set): Set of known valid words.\n",
        "        unigram_total (int): Total count of unigrams.\n",
        "        keyboard_layout (List[str], optional): List of keyboard rows (e.g., [\"qwertyuiop\"]). Default is None.\n",
        "        smoothing_factor (int, optional): Smoothing factor for Laplace smoothing. Default is 1.\n",
        "        weights (tuple, optional): Weights for (unigram, bigram, trigram) probabilities. Default is (0.1, 0.3, 0.6).\n",
        "\n",
        "    Returns:\n",
        "        str: The corrected text.\n",
        "    \"\"\"\n",
        "    words_in_text = preprocess_text(text)\n",
        "    corrected_words = []\n",
        "    context = []  # Holds the corrected words as context for subsequent words\n",
        "\n",
        "    for word in words_in_text:\n",
        "        # Generate candidate corrections for the current word\n",
        "        candidates = get_candidates(word, vocabulary, keyboard_layout)\n",
        "        # Select the candidate with the highest interpolated probability score\n",
        "        best_candidate = max(candidates, key=lambda c: probability_of_correction(\n",
        "            c, context, unigrams, bigrams, trigrams, unigram_total, smoothing_factor, weights))\n",
        "        corrected_words.append(best_candidate)\n",
        "        context.append(best_candidate)\n",
        "\n",
        "    return ' '.join(corrected_words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oML-5sJwGRLE"
      },
      "source": [
        "## Justify your decisions\n",
        "\n",
        "Write down justificaitons for your implementation choices. For example, these choices could be:\n",
        "- Which ngram dataset to use\n",
        "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
        "- Beam search parameters\n",
        "- etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Dataset and N-gram Model Building\n",
        "\n",
        "- **Using big.txt:**  \n",
        "  I started by using Norvig’s `big.txt` dataset because it’s a well-known, large corpus that provides a wide range of words and word sequences. Building the unigram, bigram, and trigram models on `big.txt` helped me get reliable frequency counts and a solid language model foundation.\n",
        "\n",
        "- **Testing with wiki_sentences_v2.csv:**  \n",
        "  After constructing the language model using `big.txt`, I decided to test my corrector on real-world data from Wikipedia using the `wiki_sentences_v2.csv` file. This dataset contains actual sentences, making it a great benchmark for evaluating the performance of my spelling corrector on noisy, realistic text.\n",
        "  You can find the dataset of Wiki Sentences [here](https://www.kaggle.com/datasets/ved1104/wiki-sentences/data).\n",
        "\n",
        "## 2. Smoothing Technique\n",
        "\n",
        "I used simple Laplace smoothing (adding a constant, typically 1) when calculating n-gram frequencies. Although this is a basic method, it was enough for handling unseen n-grams in my experiments.\n",
        "\n",
        "## 3. Candidate Generation Strategy\n",
        "\n",
        "- **Edit Distance Candidates:**  \n",
        "  My approach for candidate generation is inspired by Norvig’s method. I first check if the word is already in my vocabulary. If it isn’t, I generate candidates using edit distance 1, and if necessary, expand to edit distance 2. This method is simple and strikes a good balance between efficiency and quality.\n",
        "\n",
        "- **Handling Keyboard Errors:**  \n",
        "  Since many typos occur due to pressing adjacent keys on the keyboard, I added a module to handle keyboard layout errors (using a QWERTY layout). This helps improve correction accuracy by catching common typing mistakes.\n",
        "\n",
        "## 4. Interpolation Weights for N-gram Models\n",
        "\n",
        "I assigned weights of **(0.1, 0.3, 0.6)** to the unigram, bigram, and trigram probabilities respectively:\n",
        "\n",
        "- **Trigram (0.6):**  \n",
        "  I believe the two-word context from the trigram model provides the most valuable information, so it gets the highest weight.\n",
        "\n",
        "- **Bigram (0.3):**  \n",
        "  The immediate previous word is also important, though it’s a bit less informative than the full trigram context.\n",
        "\n",
        "- **Unigram (0.1):**  \n",
        "  Overall word frequency is considered, but it plays a smaller role compared to contextual models.\n",
        "\n",
        "I chose these weights based on my initial experiments and intuition, and I might adjust them later with more thorough testing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46rk65S4GRSe"
      },
      "source": [
        "## Evaluate on a test set\n",
        "\n",
        "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity (or just take another dataset). Compare your solution to the Norvig's corrector, and report the accuracies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word-level accuracy: 97.50%\n",
            "Sentence-level accuracy: 70.52%\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "def add_noise_to_word(word, noise_prob=0.2):\n",
        "    \"\"\"\n",
        "    With probability `noise_prob`, perturb the word by performing one of the following operations:\n",
        "      - Deletion: Remove a random character.\n",
        "      - Insertion: Insert a random character at a random position.\n",
        "      - Replacement: Replace a random character with another random character.\n",
        "      - Transposition: Swap two adjacent characters.\n",
        "      \n",
        "    If no noise is added (based on the probability), returns the original word.\n",
        "\n",
        "    Parameters:\n",
        "        word (str): The original word.\n",
        "        noise_prob (float): The probability with which to add noise. Default is 0.2.\n",
        "\n",
        "    Returns:\n",
        "        str: The potentially noised word.\n",
        "    \"\"\"\n",
        "    # With probability (1 - noise_prob) return the original word\n",
        "    if random.random() > noise_prob:\n",
        "        return word\n",
        "\n",
        "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    mod_type = random.choice(['delete', 'insert', 'replace', 'transpose'])\n",
        "    \n",
        "    if mod_type == 'delete' and len(word) > 1:\n",
        "        idx = random.randint(0, len(word) - 1)\n",
        "        return word[:idx] + word[idx+1:]\n",
        "    elif mod_type == 'insert':\n",
        "        idx = random.randint(0, len(word))\n",
        "        letter = random.choice(letters)\n",
        "        return word[:idx] + letter + word[idx:]\n",
        "    elif mod_type == 'replace' and len(word) > 0:\n",
        "        idx = random.randint(0, len(word) - 1)\n",
        "        letter = random.choice(letters)\n",
        "        return word[:idx] + letter + word[idx+1:]\n",
        "    elif mod_type == 'transpose' and len(word) > 1:\n",
        "        idx = random.randint(0, len(word) - 2)\n",
        "        return word[:idx] + word[idx+1] + word[idx] + word[idx+2:]\n",
        "    \n",
        "    return word\n",
        "\n",
        "def add_noise_to_sentence(sentence, noise_prob=0.2):\n",
        "    \"\"\"\n",
        "    Apply noise to each word in the sentence using the add_noise_to_word function.\n",
        "\n",
        "    Parameters:\n",
        "        sentence (str): The input sentence.\n",
        "        noise_prob (float): The probability of applying noise to each word. Default is 0.2.\n",
        "\n",
        "    Returns:\n",
        "        str: The sentence after noise has been applied to its words.\n",
        "    \"\"\"\n",
        "    words_in_sentence = sentence.split()\n",
        "    noisy_words = [add_noise_to_word(word, noise_prob) for word in words_in_sentence]\n",
        "    return ' '.join(noisy_words)\n",
        "\n",
        "# ----------------------\n",
        "# Load and Prepare the Dataset\n",
        "# ----------------------\n",
        "# Load the dataset from a CSV file. The CSV should contain a column named 'sentence'.\n",
        "df = pd.read_csv('wiki_sentences_v2.csv')\n",
        "\n",
        "# Create a new column with \"noisy\" sentences.\n",
        "# Each word in the original sentence is perturbed with a 20% probability.\n",
        "df['noisy_sentence'] = df['sentence'].apply(lambda s: add_noise_to_sentence(s, noise_prob=0.2))\n",
        "\n",
        "# ----------------------\n",
        "# Build Corpus and Vocabulary\n",
        "# ----------------------\n",
        "# Extract the corpus from the original sentences\n",
        "corpus = df['sentence'].tolist()\n",
        "\n",
        "# Build the vocabulary by splitting each sentence into words and converting them to lowercase\n",
        "vocabulary = set()\n",
        "for sentence in corpus:\n",
        "    vocabulary.update(sentence.lower().split())\n",
        "\n",
        "# ----------------------\n",
        "# Build N-gram Models\n",
        "# ----------------------\n",
        "# Build unigram, bigram, and trigram frequency models from the corpus.\n",
        "# The function `get_ngrams` is assumed to be defined elsewhere.\n",
        "unigrams, unigram_total = get_ngrams(corpus, 1)\n",
        "bigrams, _ = get_ngrams(corpus, 2)\n",
        "trigrams, _ = get_ngrams(corpus, 3)\n",
        "\n",
        "# ----------------------\n",
        "# Evaluate the Context-Sensitive Spelling Corrector\n",
        "# ----------------------\n",
        "total_sentences = len(df)\n",
        "word_correct = 0   # Counter for correctly corrected words\n",
        "total_words = 0    # Total number of words processed\n",
        "sentence_correct = 0  # Counter for sentences that are exactly corrected\n",
        "corrected_sentences = []  # To store the corrected sentences\n",
        "\n",
        "# Define a QWERTY keyboard layout (lowercase letters only) to account for keyboard errors.\n",
        "keyboard_layout = [\"qwertyuiop\", \"asdfghjkl\", \"zxcvbnm\"]\n",
        "\n",
        "# Iterate over each row in the DataFrame\n",
        "for index, row in df.iterrows():\n",
        "    original = row['sentence']\n",
        "    noisy = row['noisy_sentence']\n",
        "    \n",
        "    # Use the context-sensitive correct_spelling function to correct the noisy sentence.\n",
        "    corrected = correct_spelling(noisy, unigrams, bigrams, trigrams, vocabulary, unigram_total, keyboard_layout)\n",
        "    corrected_sentences.append(corrected)\n",
        "    \n",
        "    # Evaluate at the word level:\n",
        "    orig_words = original.lower().split()\n",
        "    corr_words = corrected.lower().split()\n",
        "    # Compare corresponding words (up to the length of the shorter sentence)\n",
        "    for o, c in zip(orig_words, corr_words):\n",
        "        if o == c:\n",
        "            word_correct += 1\n",
        "    total_words += len(orig_words)\n",
        "    \n",
        "    # Evaluate at the sentence level:\n",
        "    if original.lower() == corrected.lower():\n",
        "        sentence_correct += 1\n",
        "\n",
        "# Save the corrected sentences to a new column in the DataFrame.\n",
        "df['corrected_sentence'] = corrected_sentences\n",
        "\n",
        "# Print the evaluation results:\n",
        "print(\"Word-level accuracy: {:.2f}%\".format((word_correct / total_words) * 100))\n",
        "print(\"Sentence-level accuracy: {:.2f}%\".format((sentence_correct / total_sentences) * 100))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Norvig - Word-level accuracy: 78.67%\n",
            "Norvig - Sentence-level accuracy: 2.22%\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import collections\n",
        "\n",
        "# ------------------------------\n",
        "# Norvig's Spelling Corrector Setup\n",
        "# ------------------------------\n",
        "\n",
        "def words(text):\n",
        "    \"\"\"\n",
        "    Tokenize the input text by extracting all word characters in lowercase.\n",
        "    \n",
        "    Parameters:\n",
        "        text (str): The text to be tokenized.\n",
        "    \n",
        "    Returns:\n",
        "        List[str]: A list of lowercase word tokens.\n",
        "    \"\"\"\n",
        "    return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "# Build the WORDS frequency dictionary from the original sentences in the dataset.\n",
        "# This dictionary will serve as the basis for word probabilities.\n",
        "WORDS = collections.Counter()\n",
        "for sentence in df['sentence']:\n",
        "    WORDS.update(words(sentence))\n",
        "N = sum(WORDS.values())\n",
        "\n",
        "def P(word, N=N):\n",
        "    \"\"\"\n",
        "    Calculate the probability of a word based on its frequency in the WORDS dictionary.\n",
        "    \n",
        "    Parameters:\n",
        "        word (str): The word for which to calculate the probability.\n",
        "        N (int): Total count of all words.\n",
        "    \n",
        "    Returns:\n",
        "        float: The probability of the word.\n",
        "    \"\"\"\n",
        "    return WORDS[word] / N\n",
        "\n",
        "def correction(word):\n",
        "    \"\"\"\n",
        "    Return the most probable correction for the given word.\n",
        "    \n",
        "    Parameters:\n",
        "        word (str): The word to be corrected.\n",
        "    \n",
        "    Returns:\n",
        "        str: The most likely corrected word.\n",
        "    \"\"\"\n",
        "    return max(candidates(word), key=P)\n",
        "\n",
        "def candidates(word):\n",
        "    \"\"\"\n",
        "    Generate candidate corrections for the input word.\n",
        "    The candidate set includes:\n",
        "      - The word itself (if it is known),\n",
        "      - All words that are one edit distance away,\n",
        "      - All words that are two edit distances away.\n",
        "    \n",
        "    Parameters:\n",
        "        word (str): The word for which to generate candidate corrections.\n",
        "    \n",
        "    Returns:\n",
        "        set: A set of candidate words.\n",
        "    \"\"\"\n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "def known(words_list):\n",
        "    \"\"\"\n",
        "    Filter a list of words and return only those that exist in the WORDS dictionary.\n",
        "    \n",
        "    Parameters:\n",
        "        words_list (Iterable[str]): A list or iterable of words.\n",
        "    \n",
        "    Returns:\n",
        "        set: A set of words that are present in WORDS.\n",
        "    \"\"\"\n",
        "    return set(w for w in words_list if w in WORDS)\n",
        "\n",
        "def edits1(word):\n",
        "    \"\"\"\n",
        "    Generate all words that are one edit away from the input word.\n",
        "    The edits include deletion, transposition, replacement, and insertion.\n",
        "    \n",
        "    Parameters:\n",
        "        word (str): The original word.\n",
        "    \n",
        "    Returns:\n",
        "        set: A set of words one edit distance away.\n",
        "    \"\"\"\n",
        "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:]   for L, R in splits if len(R) > 1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word):\n",
        "    \"\"\"\n",
        "    Generate all words that are two edits away from the input word.\n",
        "    \n",
        "    Parameters:\n",
        "        word (str): The original word.\n",
        "    \n",
        "    Returns:\n",
        "        generator: A generator yielding words two edits away.\n",
        "    \"\"\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
        "\n",
        "def norvig_correct_spelling(sentence):\n",
        "    \"\"\"\n",
        "    Correct the spelling of each word in a sentence using Norvig's approach.\n",
        "    Each word is lowercased, and the correction is performed independently.\n",
        "    \n",
        "    Parameters:\n",
        "        sentence (str): The sentence to be corrected.\n",
        "    \n",
        "    Returns:\n",
        "        str: The corrected sentence.\n",
        "    \"\"\"\n",
        "    tokens = sentence.split()\n",
        "    corrected_tokens = [correction(token.lower()) for token in tokens]\n",
        "    return \" \".join(corrected_tokens)\n",
        "\n",
        "# ------------------------------\n",
        "# Evaluation of Norvig's Corrector on the Dataset\n",
        "# ------------------------------\n",
        "\n",
        "# Initialize counters for evaluation metrics.\n",
        "total_sentences = len(df)\n",
        "word_correct_norvig = 0         # Count of correctly corrected words.\n",
        "total_words_norvig = 0          # Total number of words evaluated.\n",
        "sentence_correct_norvig = 0     # Count of sentences that match exactly.\n",
        "norvig_corrected_sentences = [] # List to store the corrected sentences.\n",
        "\n",
        "# Iterate over each row in the dataset.\n",
        "for index, row in df.iterrows():\n",
        "    original = row['sentence']\n",
        "    noisy = row['noisy_sentence']\n",
        "    \n",
        "    # Correct the noisy sentence using Norvig's spelling corrector.\n",
        "    corrected = norvig_correct_spelling(noisy)\n",
        "    norvig_corrected_sentences.append(corrected)\n",
        "    \n",
        "    # Evaluate word-level accuracy by comparing each corresponding word.\n",
        "    orig_words = original.lower().split()\n",
        "    corr_words = corrected.lower().split()\n",
        "    for o, c in zip(orig_words, corr_words):\n",
        "        if o == c:\n",
        "            word_correct_norvig += 1\n",
        "    total_words_norvig += len(orig_words)\n",
        "    \n",
        "    # Evaluate sentence-level accuracy (exact match).\n",
        "    if original.lower() == corrected.lower():\n",
        "        sentence_correct_norvig += 1\n",
        "\n",
        "# Add the Norvig corrected sentences to the DataFrame for further analysis.\n",
        "df['norvig_corrected'] = norvig_corrected_sentences\n",
        "\n",
        "# Print the evaluation results.\n",
        "print(\"Norvig - Word-level accuracy: {:.2f}%\".format((word_correct_norvig / total_words_norvig) * 100))\n",
        "print(\"Norvig - Sentence-level accuracy: {:.2f}%\".format((sentence_correct_norvig / total_sentences) * 100))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Useful resources (also included in the archive in moodle):\n",
        "\n",
        "1. [Possible dataset with N-grams](https://www.ngrams.info/download_coca.asp)\n",
        "2. [Damerau–Levenshtein distance](https://en.wikipedia.org/wiki/Damerau–Levenshtein_distance#:~:text=Informally%2C%20the%20Damerau–Levenshtein%20distance,one%20word%20into%20the%20other.)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
